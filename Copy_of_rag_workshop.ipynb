{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meer2001/RAG-Workshop/blob/main/Copy_of_rag_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Et1ikJmoAe"
      },
      "source": [
        "# RAG Workshop with LangGraph and Gemini API\n",
        "\n",
        "## Access the tinyurl link to open the colab and make a copy of it to follow along!\n",
        "\n",
        "File > Save a Copy in Drive\n",
        "\n",
        "### https://tinyurl.com/simple-rag-ais\n",
        "\n",
        "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) system using:\n",
        "- **LangGraph** for RAG pipeline orchestration\n",
        "- **Gemini API** for text generation\n",
        "- **PDF documents** from a data directory\n",
        "- **Vector embeddings** for semantic search\n",
        "- **ChromaDB** for vector storage and retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vxmyZEQNmoAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b09510-4bb7-46a4-d826-2644579ea9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages (run this cell if packages are not installed)\n",
        "!pip install -q langchain langgraph langchain-google-genai langchain-community langchain-core langchain-text-splitters python-dotenv pypdf chromadb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "709VijbWOYwH"
      },
      "source": [
        "Use the following api key: https://pastebin.com/NJrVX2xG. To create a secret in google colab:\n",
        "1. Click on the key icon in the sidebar of the notebook\n",
        "2. Add new secret\n",
        "3. Type in gemini for the name and the value.\n",
        "4. Enable notebook access for the secret\n",
        "\n",
        "Note: The api key will be destroyed after this workshop, if you'd like to play with these scripts later, visit: https://aistudio.google.com for your own api key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AjfCoiQmoAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a7a589-b3d1-4d17-e9cd-ac96956a41ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variables loaded\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from typing import TypedDict, List\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "import glob\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load environment variables\n",
        "\n",
        "# Verify API key is loaded\n",
        "GEMINI_API_KEY = userdata.get('gemini')\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
        "\n",
        "# Also set as GOOGLE_API_KEY for LangChain compatibility\n",
        "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n",
        "\n",
        "print(\"Environment variables loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VULvRgmWmoAi"
      },
      "source": [
        "## Step 1: Load PDF Documents\n",
        "\n",
        "This step prepares the document collection for the RAG system:\n",
        "\n",
        "- **Download documents from Google Drive**: Authenticate with Google Drive API and copy the shared folder containing PDF documents to your Drive\n",
        "- **Locate PDF files**: Search for all PDF files in the specified data directory\n",
        "- **Load PDF documents**: Use PyPDFLoader to extract text content from each PDF file\n",
        "- **Add metadata**: Attach source file information to each document for traceability\n",
        "- **Display summary**: Show the total number of documents loaded and total character count across all pages\n",
        "\n",
        "Replace the text at ```shared_link = \"INSERT LINK HERE\"``` with \"https://drive.google.com/drive/folders/1iwWlvtMj-QY0usHFQGqPGDv1Vu9Y7yaq\". This is how we copy the 'data' folder from someone else's drive into our own.\n",
        "\n",
        "Make sure you allow all permissions for your drive during the mounting authentication for this to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6N0FOOrhkx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2b9b33-b8ad-4d13-ea01-afc1b8ec756c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFZ9hknpnSC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e7fb9b-0114-46f0-9c71-08604e29404c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied folder: https://drive.google.com/drive/folders/1stoaQI0b1QqDFHOUX0PLL-JvoNizsjFl\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pydrive2\n",
        "\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "service = build(\"drive\", \"v3\", credentials=gauth.credentials)\n",
        "\n",
        "def extract_id_from_url(url: str) -> str:\n",
        "    patterns = [\n",
        "        r\"/d/([a-zA-Z0-9_-]+)\",\n",
        "        r\"id=([a-zA-Z0-9_-]+)\",\n",
        "        r\"/folders/([a-zA-Z0-9_-]+)\",\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return url.strip()\n",
        "\n",
        "def copy_file(service, file_id, parent_id=None, new_name=None):\n",
        "    body = {}\n",
        "    if new_name:\n",
        "        body[\"name\"] = new_name\n",
        "    if parent_id:\n",
        "        body[\"parents\"] = [parent_id]\n",
        "\n",
        "    copied = service.files().copy(\n",
        "        fileId=file_id,\n",
        "        body=body,\n",
        "        fields=\"id, name\"\n",
        "    ).execute()\n",
        "    return copied\n",
        "\n",
        "def create_folder(service, name, parent_id=None):\n",
        "    metadata = {\n",
        "        \"name\": name,\n",
        "        \"mimeType\": \"application/vnd.google-apps.folder\"\n",
        "    }\n",
        "    if parent_id:\n",
        "        metadata[\"parents\"] = [parent_id]\n",
        "\n",
        "    folder = service.files().create(\n",
        "        body=metadata,\n",
        "        fields=\"id, name\"\n",
        "    ).execute()\n",
        "    return folder\n",
        "\n",
        "def copy_folder_recursive(service, source_folder_id, target_parent_id=None):\n",
        "    src = service.files().get(\n",
        "        fileId=source_folder_id,\n",
        "        fields=\"id, name\"\n",
        "    ).execute()\n",
        "\n",
        "    new_folder = create_folder(\n",
        "      service,\n",
        "      name=src[\"name\"],\n",
        "      parent_id=target_parent_id\n",
        "    )\n",
        "    new_folder_id = new_folder[\"id\"]\n",
        "\n",
        "    page_token = None\n",
        "    while True:\n",
        "        response = service.files().list(\n",
        "            q=f\"'{source_folder_id}' in parents and trashed=false\",\n",
        "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"files\", []):\n",
        "            if item[\"mimeType\"] == \"application/vnd.google-apps.folder\":\n",
        "                copy_folder_recursive(service, item[\"id\"], new_folder_id)\n",
        "            else:\n",
        "                copy_file(\n",
        "                    service,\n",
        "                    file_id=item[\"id\"],\n",
        "                    parent_id=new_folder_id,\n",
        "                    new_name=item[\"name\"]\n",
        "                )\n",
        "\n",
        "        page_token = response.get(\"nextPageToken\", None)\n",
        "        if page_token is None:\n",
        "            break\n",
        "\n",
        "    return new_folder\n",
        "\n",
        "shared_link = \"https://drive.google.com/drive/folders/1iwWlvtMj-QY0usHFQGqPGDv1Vu9Y7yaq\"\n",
        "\n",
        "source_id = extract_id_from_url(shared_link)\n",
        "\n",
        "meta = service.files().get(\n",
        "    fileId=source_id,\n",
        "    fields=\"id, name, mimeType\"\n",
        ").execute()\n",
        "\n",
        "mime = meta[\"mimeType\"]\n",
        "name = meta[\"name\"]\n",
        "\n",
        "if mime == \"application/vnd.google-apps.folder\":\n",
        "    new_folder = copy_folder_recursive(service, source_id, target_parent_id=None)\n",
        "    new_link = f\"https://drive.google.com/drive/folders/{new_folder['id']}\"\n",
        "    print(f\"Copied folder: {new_link}\")\n",
        "else:\n",
        "    copied = copy_file(\n",
        "        service,\n",
        "        file_id=source_id,\n",
        "        parent_id=None,\n",
        "        new_name=name\n",
        "    )\n",
        "    new_link = f\"https://drive.google.com/file/d/{copied['id']}/view\"\n",
        "    print(f\"Copied file: {new_link} into your drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7eeG29WmoAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a7bbcc-43bd-4ae9-9658-b7f16785f60c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: Ch 10 Chisquare_Test.pdf (13 pages)\n",
            "Loaded: Ch 9 Hypothesis Test.pdf (30 pages)\n",
            "Loaded: Ch 5 Simulation.pdf (12 pages)\n",
            "Loaded: Chapter 1-4 Review of Prob.pdf (121 pages)\n",
            "Loaded: Probability and statistics for computer scientists.pdf (487 pages)\n",
            "Loaded: Ch 6-graphical_statistics.pdf (17 pages)\n",
            "Loaded: Ch 7-point_estimation.pdf (14 pages)\n",
            "Loaded: Ch 8 Confidence_Intervals.pdf (43 pages)\n",
            "\n",
            "Total documents loaded: 737\n",
            "Total pages: 1019317 characters\n"
          ]
        }
      ],
      "source": [
        "# Load all PDF documents from the data directory (that is now inside of your drive)\n",
        "pdf_files = glob.glob(os.path.join(\"./drive/MyDrive/data\", \"*.pdf\"))\n",
        "\n",
        "if not pdf_files:\n",
        "    print(\"No PDF files found in data directory\")\n",
        "    documents = []\n",
        "else:\n",
        "    documents = []\n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            docs = loader.load()\n",
        "            # Add source metadata to each document\n",
        "            for doc in docs:\n",
        "                doc.metadata['source'] = os.path.basename(pdf_path)\n",
        "            documents.extend(docs)\n",
        "            print(f\"Loaded: {os.path.basename(pdf_path)} ({len(docs)} pages)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {pdf_path}: {e}\")\n",
        "\n",
        "print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
        "print(f\"Total pages: {sum([len(doc.page_content) for doc in documents])} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkwRQXnamoAi"
      },
      "source": [
        "## Step 2: Chunk Documents\n",
        "\n",
        "This step breaks down large documents into smaller, manageable pieces for efficient processing:\n",
        "\n",
        "- **Initialize text splitter**: Configure RecursiveCharacterTextSplitter with optimal chunk size (1000 characters) and overlap (200 characters) to maintain context between chunks\n",
        "- **Split documents**: Process all loaded documents and divide them into smaller text chunks\n",
        "- **Preserve structure**: Use separators (paragraphs, sentences, words) to split at natural boundaries\n",
        "- **Maintain metadata**: Keep source information attached to each chunk for proper attribution\n",
        "- **Display statistics**: Show total number of chunks created and average chunk size for verification\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmSY5TRmmoAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "3e641f55-4ca9-4624-ef97-c36e5dbca8ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1559893365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total chunks created: {len(texts)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Total chunks created: {len(texts)}\")\n",
        "print(f\"Average chunk size: {sum([len(chunk.page_content) for chunk in texts]) / len(texts):.0f} characters\")\n",
        "print(f\"\\nSample chunk:\")\n",
        "print(f\"Source: {texts[0].metadata.get('source', 'Unknown')}\")\n",
        "print(f\"Content preview: {texts[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDcI-iSfmoAj"
      },
      "source": [
        "## Step 3: Create Vector Store with ChromaDB\n",
        "\n",
        "This step converts text chunks into vector embeddings and stores them for semantic search:\n",
        "\n",
        "- **Initialize embeddings model**: Set up Google's embedding model (embedding-001) to convert text into high-dimensional vectors\n",
        "- **Generate embeddings**: Transform all document chunks into vector representations that capture semantic meaning\n",
        "- **Create vector database**: Use ChromaDB to store embeddings with their associated text chunks and metadata\n",
        "- **Persist to disk**: Save the vector store locally so it can be reused without regenerating embeddings\n",
        "- **Verify creation**: Confirm the vector store contains all document chunks and is ready for retrieval\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XfYcv8BmoAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283b0362-2c91-45b8-9e00-8cfb5c1d202d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vector store with ChromaDB...\n",
            "Vector store created with 1528 documents\n",
            "Database persisted to: ./chroma_db\n"
          ]
        }
      ],
      "source": [
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    google_api_key=GEMINI_API_KEY\n",
        ")\n",
        "\n",
        "print(\"Creating vector store with ChromaDB...\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=texts,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "# Load existing vector store (uncomment to use instead of creating new)\n",
        "# vectorstore = Chroma(\n",
        "#     persist_directory=\"./chroma_db\",\n",
        "#     embedding_function=embeddings\n",
        "# )\n",
        "\n",
        "print(f\"Vector store created with {len(texts)} documents\")\n",
        "print(f\"Database persisted to: ./chroma_db\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKPW10V2moAj"
      },
      "source": [
        "## Step 4: Initialize Gemini LLM\n",
        "\n",
        "This step sets up the language model that will generate answers based on retrieved context:\n",
        "\n",
        "- **Configure Gemini model**: Initialize the ChatGoogleGenerativeAI client with the gemini-flash-latest model\n",
        "- **Set temperature**: Configure temperature (0-1) to balance between focused and creative responses\n",
        "- **Handle system messages**: Enable conversion of system messages to human messages for compatibility\n",
        "- **Authenticate**: Provide API key for secure access to Google's Gemini API\n",
        "- **Verify initialization**: Confirm the LLM is ready to process queries and generate responses\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kIz2Xn_moAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543fca78-5884-48d3-9df8-2a9fe9b094d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini LLM initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize Gemini LLM\n",
        "# Explicitly pass the API key to avoid using Application Default Credentials\n",
        "# gemini-flash-latest model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-flash-latest\",  # Updated to current model name\n",
        "    temperature=1,\n",
        "    convert_system_message_to_human=True,\n",
        "    google_api_key=GEMINI_API_KEY\n",
        ")\n",
        "\n",
        "print(\"Gemini LLM initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMG4Dj3moAj"
      },
      "source": [
        "## Step 5: Create RAG Graph with LangGraph\n",
        "\n",
        "This step assembles the complete RAG pipeline as an explicit graph using LangGraph:\n",
        "\n",
        "- **Design prompt template**: Create a comprehensive prompt that instructs the LLM to answer questions using retrieved context while allowing for interpretation and synthesis\n",
        "- **Define state schema**: Create a `RAGState` TypedDict with `question`, `context`, and `answer` fields that flow through the graph\n",
        "- **Configure retriever**: Set up the vector store retriever to fetch the top 3 most relevant document chunks for each query\n",
        "- **Create node functions**: Define a `retrieve` node (fetches relevant docs from the vector store) and a `generate` node (formats context, prompts the LLM, and extracts the answer)\n",
        "- **Build the graph**: Use LangGraph's `StateGraph` to wire nodes together with edges: `retrieve` -> `generate` -> `END`\n",
        "- **Compile the graph**: Call `workflow.compile()` to produce a runnable graph that processes queries end-to-end\n",
        "\n",
        "Follow along with the workshop and replace the ```\"INSERT\"``` quotes with actual code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwm7e9UamoAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b7982a-8ca4-4b38-a8e4-02b5dcfebdd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph RAG pipeline created and ready to use\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import TypedDict, List\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "import glob\n",
        "from google.colab import userdata\n",
        "\n",
        "# Create a custom prompt template for the RAG graph\n",
        "# This prompt allows for interpretation and reasoning while staying grounded in the documents\n",
        "prompt_template = \"\"\"You are an expert assistant helping to answer questions based on the provided context from documents.\\n\\nYour task is to:\\n1. Use the context as the foundation for your answer\\n2. If the context doesn't contain enough information, clearly state what is missing\\n\\nContext from documents:\\n{context}\\n\\nQuestion: {question}\\n\\nInstructions:\\n- Base your answer on the provided context, but feel free to interpret, explain, and make logical connections\\n- If the context is insufficient, say you cannot answer the question.\\n\\nProvide a thoughtful, well-reasoned answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "def retrieve(state: RAGState) -> dict:\n",
        "    \"\"\"Retrieve relevant documents based on the question.\"\"\"\n",
        "    docs = retriever.invoke(state[\"question\"])\n",
        "    return {\"context\": docs}\n",
        "\n",
        "def generate(state: RAGState) -> dict:\n",
        "    \"\"\"Format context, prompt the LLM, and return the answer.\"\"\"\n",
        "    formatted_context = \"\\n\\n\".join([\n",
        "        f\"[From {doc.metadata.get('source', 'Unknown')}]\\n{doc.page_content}\"\n",
        "        for doc in state[\"context\"]\n",
        "    ])\n",
        "    prompt = PROMPT.format(context=formatted_context, question=state[\"question\"])\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "workflow = StateGraph(RAGState)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "rag_graph = workflow.compile()\n",
        "\n",
        "print(\"LangGraph RAG pipeline created and ready to use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwRwWDPOmoAk"
      },
      "source": [
        "## Step 6: Test Retrieval\n",
        "\n",
        "This step validates that the retrieval system is working correctly before generating answers:\n",
        "\n",
        "- **Define test query**: Create a sample question to evaluate the retrieval functionality\n",
        "- **Execute retrieval**: Query the vector store to find the most semantically similar document chunks\n",
        "- **Display results**: Show the retrieved documents with their source files and content previews\n",
        "- **Verify relevance**: Check that the retrieved chunks are actually relevant to the test query\n",
        "- **Confirm functionality**: Ensure the retrieval system is finding appropriate context for questions\n",
        "\n",
        "Follow along with the workshop and try out different test queries based off the data in the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbnyCAySmoAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19d2257-426f-4009-df1c-c1d63ccb8fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 documents for query: 'What are Monte Carlo methods?'\n",
            "\n",
            "1. From: Probability and statistics for computer scientists.pdf\n",
            "   Content preview: complicated, risky, consuming, expensive, or impossible. For example, suppose a complex\n",
            "device or machine is to be built and launched. Before it happens, its performance is sim-\n",
            "ulated, and this allows experts to evaluate its adequacy and associated risks carefully and\n",
            "safely. For example, one surely prefers to evaluate reliability and safety of a new module of\n",
            "a space station by means of computer simulations rather than during the actual mission.\n",
            "Monte Carlo methods are mostly used for the comp...\n",
            "\n",
            "2. From: Probability and statistics for computer scientists.pdf\n",
            "   Content preview: 104 Probability and Statistics for Computer Scientists\n",
            "FIGURE 5.1:Casino Monte Carlo in Principality\n",
            "of Monaco.\n",
            "Monte Carlo methods and Monte Carlo\n",
            "studies inherit their name from Eu-\n",
            "rope’s most famousMonte Carlo casino\n",
            "(Figure 5.1) located in Principality of\n",
            "Monaco on the Mediterranean coast\n",
            "since the 1850s. Probability distribu-\n",
            "tions involved in gambling are often\n",
            "complicated, but they can be assessed\n",
            "via simulations. In early times, math-\n",
            "ematicians generated extra income by\n",
            "estimating vita...\n",
            "\n",
            "3. From: Ch 5 Simulation.pdf\n",
            "   Content preview: Solving problems by Monte Carlo methods\n",
            "Estimating µ =E(X) and σ2 =var(X) =E(X−µ)2:\n",
            "Simulate a large number (N) of independent draws from the\n",
            "distribution of X, say,X1,X 2,...,X N\n",
            "MC estimator of µ:\n",
            "MC estimator of E[g(X)] whereg is a given function:\n",
            "MC estimator of σ2:\n",
            "6 / 12...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test retrieval to see what documents are found\n",
        "test_query = \"What are Monte Carlo methods?\"\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Use invoke for newer LangChain versions, fallback to get_relevant_documents\n",
        "try:\n",
        "    retrieved_docs = retriever.invoke(test_query)\n",
        "except AttributeError:\n",
        "    retrieved_docs = retriever.get_relevant_documents(test_query)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents for query: '{test_query}'\\n\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    source = doc.metadata.get('source', 'Unknown')\n",
        "    print(f\"{i}. From: {source}\")\n",
        "    print(f\"   Content preview: {doc.page_content[:500]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziAqKkHsmoAk"
      },
      "source": [
        "## Step 7: RAG Query Function\n",
        "\n",
        "This step creates a user-friendly function to query the RAG system:\n",
        "\n",
        "- **Define query function**: Create a `rag_query()` function that handles the complete RAG workflow\n",
        "- **Retrieve context**: Automatically fetch relevant document chunks based on the user's question\n",
        "- **Generate answer**: Use the LLM to synthesize an answer from the retrieved context\n",
        "- **Display results**: Present the question, answer, and optionally the source documents used\n",
        "- **Format output**: Organize the response in a readable format with clear separation between question, answer, and sources\n",
        "- **Test functionality**: Run a sample query to demonstrate the complete RAG system in action\n",
        "\n",
        "Follow along with the workshop and try out different test queries based off the data in the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VghW8QitmoAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "3f92f159-8754-416b-f306-b86fe7136e4d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'content'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-644779775.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Test RAG query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtest_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What are the key points discussed in the documents?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mrag_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-644779775.py\u001b[0m in \u001b[0;36mrag_query\u001b[0;34m(query, show_sources)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mDictionary\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m'answer'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'context'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_sources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3072\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2647\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3026844759.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROMPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatted_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"INSERT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mworkflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAGState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
          ]
        }
      ],
      "source": [
        "def rag_query(query: str, show_sources: bool = True):\n",
        "    \"\"\"\n",
        "    Perform a RAG query: retrieve relevant context and generate answer using Gemini.\n",
        "\n",
        "    Uses the LangGraph RAG pipeline to retrieve documents and generate an answer.\n",
        "\n",
        "    Args:\n",
        "        query: The user's question\n",
        "        show_sources: Whether to show source documents\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'answer' and 'context'\n",
        "    \"\"\"\n",
        "    result = rag_graph.invoke({\"question\": query})\n",
        "\n",
        "    if show_sources:\n",
        "        print(f\"Question: {query}\\n\")\n",
        "        print(f\"Answer:\\n{result['answer']}\\n\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Source Documents:\")\n",
        "        for i, doc in enumerate(result['context'], 1):\n",
        "            source = doc.metadata.get('source', 'Unknown')\n",
        "            print(f\"\\n{i}. Source: {source}\")\n",
        "            print(f\"   Preview: {doc.page_content[:150]}...\")\n",
        "    else:\n",
        "        print(f\"Q: {query}\\n\")\n",
        "        print(f\"A: {result['answer']}\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# Test RAG query\n",
        "test_question = \"What are the key points discussed in the documents?\"\n",
        "rag_query(test_question)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD5wi2aMmoAk"
      },
      "source": [
        "## Step 8: Interactive Query Interface\n",
        "\n",
        "This step creates an interactive command-line interface for continuous querying:\n",
        "\n",
        "- **Create interactive loop**: Build a function that allows users to ask multiple questions in a session\n",
        "- **Handle user input**: Accept questions from the user via command-line input\n",
        "- **Process queries**: Execute RAG queries for each user question and display results\n",
        "- **Manage session**: Provide commands to exit the interactive mode (quit/exit/q)\n",
        "- **Error handling**: Catch and display any errors that occur during query processing\n",
        "- **User experience**: Provide clear instructions and formatting for a smooth interaction experience\n",
        "\n",
        "Run the cell, ask questions, and quit when done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slhL0eRomoAk"
      },
      "outputs": [],
      "source": [
        "def interactive_rag():\n",
        "    \"\"\"\n",
        "    Interactive function to ask questions and get RAG-based answers.\n",
        "    \"\"\"\n",
        "    print(\"RAG System Ready! Ask questions about the documents.\")\n",
        "    print(\"Type 'quit' or 'exit' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Your question: \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = rag_query(query, show_sources=True)\n",
        "            print(\"-\" * 80 + \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\\n\")\n",
        "\n",
        "\n",
        "interactive_rag()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}